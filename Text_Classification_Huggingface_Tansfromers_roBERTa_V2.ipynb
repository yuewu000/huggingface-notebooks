{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF based Transformer Embedding Aggregation from word to sentence level for MultiLabel Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, we will develop sentence level embedding based on Huggingface Transformers BERT pre-trained model. We will provide two types of aggregation methods from word level to sentence level, one by regular averaging, the other by weighted average by IDF scores for each token.\n",
    "We also generate a set of embeddings using Wiki Plot Movie datasets, and a demo with multiclass classification example.\n",
    "\n",
    "\n",
    "#### Flow of the notebook\n",
    "\n",
    "The notebook has the following sections:\n",
    "\n",
    "1. [Importing Python Libraries](#section01)\n",
    "2. [Defining modules](#section02)\n",
    "3. [Loading and processing the Dataset](#section03)\n",
    "4. [Generating sentence level embedding: two sets with diff agg methods](#section04)\n",
    "5. [Building Classification Models](#section05)\n",
    "6. [Comparing Classification Models](#section06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, gc, re, random\n",
    "from collections import defaultdict, Counter\n",
    "import copy\n",
    "import pickle\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "import spacy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import plotly.express as px\n",
    "# import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processWikiPlotData(df):\n",
    "    \n",
    "    proc_df = df[(df[\"Origin/Ethnicity\"]==\"American\") | (df[\"Origin/Ethnicity\"]==\"British\")]\n",
    "    proc_df = proc_df[[\"Plot\", \"Genre\"]]\n",
    "    drop_indices = proc_df[proc_df[\"Genre\"] == \"unknown\" ].index\n",
    "    proc_df.drop(drop_indices, inplace=True)\n",
    "    # Combine genres: 1) \"sci-fi\" with \"science fiction\" &  2) \"romantic comedy\" with \"romance\"\n",
    "    proc_df[\"Genre\"].replace({\"sci-fi\": \"science fiction\", \"romantic comedy\": \"romance\"}, inplace=True)\n",
    "\n",
    "    # Choosing movie genres based on their frequency\n",
    "    shortlisted_genres = proc_df[\"Genre\"].value_counts().reset_index(name=\"count\").query(\"count > 200\")[\"index\"].tolist()\n",
    "    proc_df = proc_df[proc_df[\"Genre\"].isin(shortlisted_genres)].reset_index(drop=True)\n",
    "\n",
    "    # Shuffle DataFrame\n",
    "    proc_df = proc_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    proc_df[\"genre_encoded\"] = label_encoder.fit_transform(proc_df[\"Genre\"].tolist())\n",
    "\n",
    "    proc_df = proc_df[[\"Plot\", \"Genre\", \"genre_encoded\"]]\n",
    "    return proc_df\n",
    "\n",
    "def partitionList(sentList, n):\n",
    "    if n == 0:\n",
    "        return [sentList[:]]\n",
    "    chunkSize = int(len(sentList)/n)\n",
    "    remainder = len(sentList) % n\n",
    "    currentIndex = 0\n",
    "    res = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if i < remainder:\n",
    "            res.append(sentList[currentIndex: currentIndex+chunkSize+1])\n",
    "            currentIndex = currentIndex+chunkSize+1\n",
    "        else:\n",
    "            res.append(sentList[currentIndex: currentIndex+chunkSize])\n",
    "            currentIndex = currentIndex+chunkSize\n",
    "    return res\n",
    "\n",
    "def genInputs4preBERT(df, textCol='Plot', labelCol='genre_encoded', chunkSize=1000):\n",
    "    sentList = []\n",
    "    labelList = []\n",
    "    for indx, row in df.iterrows():\n",
    "        sentList.append(row[textCol])\n",
    "        labelList.append(row[labelCol])\n",
    "    return sentList, partitionList(sentList, int(len(sentList)/chunkSize)), labelList\n",
    "\n",
    "def flatenListOfLists(ListOfLists, NumIters=3):\n",
    "    \"\"\"\n",
    "    Flatten the list of list by two stage to guarantee performance on databricks\n",
    "    :param ListOfLists: Input\n",
    "    :param NumIters: number of iterations, exponential growing on chunk size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if NumIters <= 0:\n",
    "        raise ValueError('Iteration number should be at least 1')\n",
    "    chunkSize = int(len(ListOfLists) ** (1/NumIters)) + 1\n",
    "    print ('Chunk size={}'.format(chunkSize))\n",
    "    currList = copy.deepcopy(ListOfLists)\n",
    "\n",
    "    for _ in range(NumIters-1):\n",
    "        tempList = []\n",
    "        loc = 0\n",
    "        print ('Current length of the list={}'.format(len(currList)))\n",
    "        while loc < len(currList):\n",
    "            tempList.append(list(functools.reduce(lambda a, b: a+b, currList[loc:loc+chunkSize])))\n",
    "            loc += chunkSize\n",
    "        currList = copy.deepcopy(tempList)\n",
    "\n",
    "    return list(functools.reduce(lambda a, b: a+b, currList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genIdfDict4BERTToks(tokenizer, sentList, saveObj = True):\n",
    "    \"\"\"\n",
    "    Map from BERT token to IDF score\n",
    "        - IDF scores by Sklearn\n",
    "    Create a dictionary mapping each token in the text corpus\n",
    "        to the idf score \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Get a dictionary of desired token to its IDF score\n",
    "    idfDict = defaultdict(float)\n",
    "\n",
    "    tokensList = list(map(lambda line: \" \".join(tokenizer.tokenize(line, tokens = tokenizer.tokenize(sentence, do_lower_case=False))), sentList))\n",
    "    tokenSet = set(flatenListOfLists(tokensList))    \n",
    "  \n",
    "    tfIdfVectorizer = TfidfVectorizer(stop_words=None, use_idf=True)\n",
    "    T = tfIdfVectorizer.fit_transform(tokensList)\n",
    "    tfidf_features = tfIdfVectorizer.get_feature_names()\n",
    "\n",
    "    # Get a dictionary of desired token to its IDF score\n",
    "    idfDict = defaultdict(float)\n",
    "    start = time()\n",
    "    L = len(tokensList)\n",
    "    print ('tokSet count={}'.format(L))\n",
    "    for i, toks in enumerate(tokensList):\n",
    "        if i % 500 == 0:\n",
    "            print ('Indx={} in {}s out of {}'.format(i, time() - start, L))\n",
    "\n",
    "        tokCounter = Counter(toks.split(' '))\n",
    "        #print(tokCounter)\n",
    "        \n",
    "        numValids = np.sum([tokCounter[tok] for tok in tokCounter if tok in tfidf_features])\n",
    "        for tok in tokCounter:\n",
    "            c = tokCounter[tok]\n",
    "            #print (c)\n",
    "            if tok in idfDict or c == 0:\n",
    "                continue\n",
    "\n",
    "            #print(tok)\n",
    "            try:\n",
    "                currIDF = T[i, tfidf_features.index(tok)] * numValids / c\n",
    "                idfDict[tok] = currIDF\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if saveObj:\n",
    "        fName = \"./dicts/idfDict.pkl\"\n",
    "        with open(fName, 'wb') as f:\n",
    "            pickle.dump(\n",
    "                idfDict,\n",
    "                f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggHFTembByMaskV2(embeddings, masks):\n",
    "    \"\"\"\n",
    "    Function to aggregate word embedding to sentence embedding\n",
    "        for Hugging Face Transformer\n",
    "    :param embeddings: Embedding tensors, multi dimension(word-level) for\n",
    "        each sentence\n",
    "    :param masks: mask tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.to('cpu').numpy()\n",
    "    masks = masks.to('cpu').numpy()\n",
    "    print(list(zip(embeddings, masks))[0][0].shape)\n",
    "    #print(list(zip(embeddings, masks))[0])\n",
    "\n",
    "    maskedVectors = [e[1:np.sum(m)-1] for e, m in zip(embeddings, masks)]\n",
    "\n",
    "    #print([(len(x), len(x[0])) for x in maskedVectors])\n",
    "    return [np.mean(vecs, axis=0) for vecs in maskedVectors]\n",
    "\n",
    "def aggHFTembByIDF(embeddings, weights):\n",
    "    \"\"\"\n",
    "    Function to aggregate word embedding to sentence embedding\n",
    "        for Hugging Face Transformer\n",
    "    :param embeddings: Embedding tensors, multi dimension(word-level) for\n",
    "        each sentence\n",
    "    :param masks: mask tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print (weights)\n",
    "    embeddings = embeddings.to('cpu').numpy()\n",
    "    #print (embeddings.shape, np.array(weights).shape)\n",
    "    maskedVectors = [(e, m) for e, m in zip(embeddings, np.array(weights))]\n",
    "    #print([(len(x), len(x[0])) for x in maskedVectors])\n",
    "\n",
    "    return [np.average(vecs, axis=0, weights=weights) for vecs, weights in maskedVectors]\n",
    "\n",
    "def encodeHFTwWeightByIDF(\n",
    "        tokenizer, model, sentences, idfDict, batch_size=512, maxLen=512,\n",
    "        show_progress_bar=None, device=torch.device('cuda'),\n",
    "        is_parallel=True):\n",
    "    \"\"\"\n",
    "    Pretraine HFT BERT embedding\n",
    "    Agg BERT embedding from token level to sentence level\n",
    "        - Regular average vector and weighted average by token IDF scores\n",
    "    Computes sentence embeddings based on pretrained BERT model:\n",
    "        tokenize with max length\n",
    "        Default model is roBERTa base\n",
    "    :param tokenizer:\n",
    "       Hugging Face tokenizer\n",
    "    :param model:\n",
    "       Hugging Face model\n",
    "    :param sentences:\n",
    "       the sentences to embed\n",
    "    :param batch_size:\n",
    "       batch_size for embedding\n",
    "    :param maxLen:\n",
    "       max Sentence length including [CLS] [SEP]\n",
    "    :param batch_size:\n",
    "       the batch size used for the computation\n",
    "    :param show_progress_bar:\n",
    "        Output a progress bar when encode sentences\n",
    "    :param device:\n",
    "       GPU or CPU\n",
    "    :param is_parallel:\n",
    "       GPU required for is_parallel==True\n",
    "    :return:\n",
    "       a list with ndarrays of the embeddings for each sentence\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    if show_progress_bar is None:\n",
    "        show_progress_bar = (\n",
    "                logging.getLogger().getEffectiveLevel() == logging.INFO or\n",
    "                logging.getLogger().getEffectiveLevel() == logging.DEBUG)\n",
    "\n",
    "    all_embeddingsM = []\n",
    "    all_embeddingsW = []\n",
    "    length_sorted_idx = np.argsort([len(sen.split()) for sen in sentences])\n",
    "\n",
    "    iterator = range(0, len(sentences), batch_size)\n",
    "    if show_progress_bar:\n",
    "        iterator = tqdm(iterator, desc=\"Batches\")\n",
    "\n",
    "    tok_cnt = 0\n",
    "\n",
    "    for batch_idx in iterator:\n",
    "        batch_tokens = []\n",
    "        batch_masks = []\n",
    "        batch_weights = []\n",
    "\n",
    "        batch_start = batch_idx\n",
    "        batch_end = min(batch_start + batch_size, len(sentences))\n",
    "        #print (\"Batch End\", batch_end)\n",
    "        longest_seq = 0\n",
    "\n",
    "        # Get the maximal token length\n",
    "        for idx in length_sorted_idx[batch_start: batch_end]:\n",
    "            sentence = sentences[idx]\n",
    "            try:\n",
    "                tempTK = tokenizer.encode(sentence)\n",
    "                tempLen = len(tempTK)\n",
    "            except:\n",
    "                tempLen = maxLen\n",
    "            longest_seq = np.max([longest_seq, tempLen])\n",
    "\n",
    "        maxLen4Batch = np.min([longest_seq, maxLen])\n",
    "        for idx in length_sorted_idx[batch_start: batch_end]:\n",
    "            sentence = sentences[idx]\n",
    "            tokens = tokenizer.tokenize(sentence, do_lower_case=False)\n",
    "            tokens = ['[CLS]'] + tokens[:maxLen4Batch-2] + ['[SEP]']\n",
    "            padded_tokens = tokens + [\n",
    "                '[PAD]' for _ in range(maxLen4Batch - len(tokens))]\n",
    "            # Get attention masks for each sentence\n",
    "            specialToks = ['[CLS]', '[SEP]', '[PAD]']\n",
    "            attn_mask = [\n",
    "                1 if token not in specialToks\n",
    "                else 0\n",
    "                for token in padded_tokens]\n",
    "\n",
    "            attn_weights = [0 for _ in range(len(attn_mask))]\n",
    "\n",
    "            for i, tok in enumerate(padded_tokens):\n",
    "                if attn_mask[i] == 1:\n",
    "                    attn_weights[i] = idfDict[tok]\n",
    "\n",
    "                    #print (padded_tokens)\n",
    "            #print (attn_weights)\n",
    "\n",
    "            sumWeights = np.sum(attn_weights)\n",
    "            if sumWeights == 0:\n",
    "                attn_weights = [1 for _ in range(len(attn_weights))]\n",
    "            attn_weights = [x/sumWeights for x in attn_weights]\n",
    "\n",
    "            # Get BERT vocabulary index for each token\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "            batch_tokens.append(token_ids)\n",
    "            batch_masks.append(attn_mask)\n",
    "            batch_weights.append(attn_weights)\n",
    "\n",
    "        tok_cnt += len(batch_tokens)\n",
    "        batch_tokens = torch.tensor(batch_tokens).to(device)\n",
    "        batch_masks = torch.tensor(batch_masks).to(device)\n",
    "\n",
    "        if len(batch_weights) == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            #Feed to pretrained BERT model for embedding\n",
    "            embeddings = torch.nn.parallel.data_parallel(\n",
    "                model, batch_tokens,\n",
    "                module_kwargs={'attention_mask': batch_masks})[0] \\\n",
    "                if is_parallel \\\n",
    "                else model.forward(\n",
    "                batch_tokens, attention_mask=batch_masks)[0]\n",
    "\n",
    "            # Regular Avg: input torch tensors; output list of vectors\n",
    "            embeddingsM = aggHFTembByMaskV2(embeddings, batch_masks)\n",
    "\n",
    "            # Weighted Avg: input torch tensors; output list of vectors\n",
    "            embeddingsW = aggHFTembByIDF(embeddings, batch_weights)\n",
    "\n",
    "            #print(batch_tokens.size, len(embeddingsM))\n",
    "            all_embeddingsW.extend(embeddingsW)\n",
    "            all_embeddingsM.extend(embeddingsM)\n",
    "        \n",
    "        #print (tok_cnt, len(all_embeddingsW))\n",
    "\n",
    "    print (\"Len all_embeddingsW = {}, Len all_embeddingsM  = {}, Len reverting_order = {}\"\n",
    "           .format(len(all_embeddingsW), len(all_embeddingsM), len(length_sorted_idx)))\n",
    "    reverting_order = np.argsort(length_sorted_idx)\n",
    "    all_embeddingsW = [all_embeddingsW[idx] for idx in reverting_order]\n",
    "    all_embeddingsM = [all_embeddingsM[idx] for idx in reverting_order]\n",
    "\n",
    "    return all_embeddingsM, all_embeddingsW\n",
    "\n",
    "def UnitVect(vect):\n",
    "    \"\"\"\n",
    "    Normalize to Unit Vector\n",
    "    :param vect: The vector to normalize\n",
    "    :return: normalized vector with length 1\n",
    "    \"\"\"\n",
    "    #\n",
    "    mag = np.linalg.norm(vect)\n",
    "\n",
    "    # Make sure we don't divide by zero!\n",
    "    if mag > 0:\n",
    "        return vect/mag\n",
    "    elif np.sum(vect) == 0:\n",
    "        return vect\n",
    "    elif math.isnan(mag):\n",
    "        #print(vect)\n",
    "        return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train he LGB model\n",
    "def trainLGB(df, \n",
    "             xCol=\"norm_embs_mean\", \n",
    "             yCol=\"labels\",\n",
    "             RANDOM_STATE = 99,\n",
    "             num_class = 16,\n",
    "             learning_rate = 0.01,\n",
    "             num_leaves = 15,\n",
    "             min_data_in_leaf = 200,\n",
    "             feature_fraction = 0.6,\n",
    "             bagging_fraction = 0.9,\n",
    "             num_boost_round = 2000,\n",
    "             max_depth = 9,\n",
    "             bagging_freq = 7,\n",
    "             early_stopping_rounds = 30):\n",
    "    \n",
    "    # train ligthGBM model\n",
    "    params = {\n",
    "          \"objective\" : \"multiclass\",\n",
    "          \"num_class\" : num_class,\n",
    "          \"num_leaves\" : num_leaves,\n",
    "          \"max_depth\": max_depth,\n",
    "          \"learning_rate\" : learning_rate,\n",
    "          \"bagging_fraction\" : bagging_fraction,  # subsample\n",
    "          \"feature_fraction\" : feature_fraction,  # colsample_bytree\n",
    "          \"bagging_freq\" : bagging_freq,        # subsample_freq\n",
    "          \"bagging_seed\" : RANDOM_STATE,\n",
    "          \"verbosity\" : -1 }\n",
    "    \n",
    "    X = featureDFedt[xCol].tolist()\n",
    "    l = featureDFedt[\"labels\"].tolist()\n",
    "    X_train, X_validate, label_train, label_validate = train_test_split(X, l, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # transform the training dataset for oversampling\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X_train, label_train)\n",
    "    \n",
    "    dtrain = lgbm.Dataset(np.array([list(v) for v in X]), y)\n",
    "    dvalid = lgbm.Dataset(np.array([list(v) for v in X_validate]), label_validate, reference=dtrain)\n",
    "    \n",
    "    lgbM = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, verbose_eval=25,\n",
    "                            early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    cm = confusion_matrix(label_validate, [np.argmax(cy) for cy in lgbM.predict(X_validate)])\n",
    "    #print(cm)\n",
    "    \n",
    "    return lgbM, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wiki Movie Plot dataset from \"https://www.kaggle.com/jrobischon/wikipedia-movie-plots\"\n",
    "# Load dataframe\n",
    "fPath = './data/wiki_movie_plots_deduped.csv copy.zip'\n",
    "df = pd.read_csv(fPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "movies_df = processWikiPlotData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat inputs for pre-trained BERT encoder\n",
    "sentList, chunks, labels = genInputs4preBERT(movies_df, chunkSize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating sentence level embedding: two sets with diff agg methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "# Load roBERTa tokens and model\n",
    "# Initial tokenizer and model\n",
    "modelType = 'roberta-base'\n",
    "print('Loading tokenizer')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(modelType)\n",
    "print('Loading pretrained model')\n",
    "model = RobertaModel.from_pretrained(modelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate token based IDF scores\n",
    "idfDict = genIdfDict4BERTToks(tokenizer, sentList, saveObj = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fName = './dicts/idfDict.pkl'\n",
    "with open(fName, 'rb') as f:\n",
    "    idfDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Aggregated sentence embedding and save by chunk\n",
    "# Mean and Weighted Avg by IDF scores\n",
    "for i in range(len(chunks)):\n",
    "    fName = \"./embs/roBERTa_embs_part{}.pkl\".format(i+1)\n",
    "    curEmbs = encodeHFTwWeightByIDF(\n",
    "        tokenizer, model, chunks[i], idfDict, batch_size=64, maxLen=512,\n",
    "        show_progress_bar=None, device=torch.device('cpu'),\n",
    "        is_parallel=False)\n",
    "    print (\"Dump part {} to {}\".format(i+1, fName))\n",
    "    with open(fName, 'wb') as f:\n",
    "        pickle.dump(\n",
    "            curEmbs,\n",
    "            f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load part 1 from ./embs/roBERTa_embs_part1.pkl\n",
      "Load part 2 from ./embs/roBERTa_embs_part2.pkl\n",
      "Load part 3 from ./embs/roBERTa_embs_part3.pkl\n",
      "Load part 4 from ./embs/roBERTa_embs_part4.pkl\n",
      "Load part 5 from ./embs/roBERTa_embs_part5.pkl\n",
      "Load part 6 from ./embs/roBERTa_embs_part6.pkl\n",
      "Load part 7 from ./embs/roBERTa_embs_part7.pkl\n",
      "Load part 8 from ./embs/roBERTa_embs_part8.pkl\n",
      "Load part 9 from ./embs/roBERTa_embs_part9.pkl\n",
      "Load part 10 from ./embs/roBERTa_embs_part10.pkl\n",
      "Load part 11 from ./embs/roBERTa_embs_part11.pkl\n",
      "Load part 12 from ./embs/roBERTa_embs_part12.pkl\n",
      "Load part 13 from ./embs/roBERTa_embs_part13.pkl\n",
      "Load part 14 from ./embs/roBERTa_embs_part14.pkl\n",
      "Load part 15 from ./embs/roBERTa_embs_part15.pkl\n",
      "15192 15192 15192\n"
     ]
    }
   ],
   "source": [
    "# Load embedding and merge to labels\n",
    "rawEmbsMean = []\n",
    "rawEmbsIdfW = []\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    fName = \"./embs/roBERTa_embs_part{}.pkl\".format(i+1)\n",
    "\n",
    "    print (\"Load part {} from {}\".format(i+1, fName))\n",
    "    with open(fName, 'rb') as f:\n",
    "        currEmbs = pickle.load(f)\n",
    "        \n",
    "    rawEmbsMean.extend(currEmbs[0])\n",
    "    rawEmbsIdfW.extend(currEmbs[1])\n",
    "    \n",
    "# Check sizes\n",
    "print (len(rawEmbsMean), len(rawEmbsIdfW), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DF and normalize the embeddings\n",
    "cols = [\"raw_embs_mean\", \"raw_embs_idfw\", \"labels\"]\n",
    "embDicts = {}\n",
    "for i, col in enumerate(cols):\n",
    "    embDicts[col] = [rawEmbsMean, rawEmbsIdfW, labels][i]\n",
    "featureDF = pd.DataFrame(embDicts)\n",
    "featureDF[\"norm_embs_mean\"] = featureDF[\"raw_embs_mean\"].apply(UnitVect)\n",
    "featureDF[\"norm_embs_idfw\"] = featureDF[\"raw_embs_idfw\"].apply(UnitVect)\n",
    "\n",
    "# Drop rows with nan in case\n",
    "featureDFedt = featureDF.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     4212\n",
       "3     3676\n",
       "8      924\n",
       "15     848\n",
       "13     669\n",
       "12     575\n",
       "11     568\n",
       "1      508\n",
       "0      505\n",
       "4      491\n",
       "9      441\n",
       "5      432\n",
       "7      340\n",
       "10     297\n",
       "14     251\n",
       "2      231\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDFedt['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_embs_mean</th>\n",
       "      <th>raw_embs_idfw</th>\n",
       "      <th>labels</th>\n",
       "      <th>norm_embs_mean</th>\n",
       "      <th>norm_embs_idfw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.02154252, 0.11547888, 0.0057376195, -0.1626...</td>\n",
       "      <td>[0.03090267432090993, 0.17266593380477255, -0....</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0018537773, 0.009937191, 0.00049373374, -0....</td>\n",
       "      <td>[0.0027959669420421166, 0.015622215667215474, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.042925723, 0.15544622, -0.024269631, -0.055...</td>\n",
       "      <td>[0.03744219678006476, 0.20829259954964385, -0....</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0036439227, 0.013195678, -0.0020602255, -0....</td>\n",
       "      <td>[0.0036025583103554406, 0.020041191490469207, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.031127831, 0.08727137, 0.0095834015, -0.071...</td>\n",
       "      <td>[-0.006864594470996933, 0.11324549126897607, -...</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0026951316, 0.0075561902, 0.0008297567, -0....</td>\n",
       "      <td>[-0.0006629951757092065, 0.010937458097396727,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.013757537, 0.08381358, 0.04083967, -0.06534...</td>\n",
       "      <td>[-0.0077409092613638765, 0.14388482091298502, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0012161772, 0.0074091866, 0.0036102592, -0....</td>\n",
       "      <td>[-0.0007445580384305126, 0.013839536984835072,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.027874012, 0.13634484, -0.03355529, -0.1006...</td>\n",
       "      <td>[-0.0033498941571190985, 0.1748613285128432, -...</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.002370487, 0.011595162, -0.0028536397, -0.0...</td>\n",
       "      <td>[-0.00031487784583386625, 0.016436327794046936...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       raw_embs_mean  \\\n",
       "0  [0.02154252, 0.11547888, 0.0057376195, -0.1626...   \n",
       "1  [0.042925723, 0.15544622, -0.024269631, -0.055...   \n",
       "2  [0.031127831, 0.08727137, 0.0095834015, -0.071...   \n",
       "3  [0.013757537, 0.08381358, 0.04083967, -0.06534...   \n",
       "4  [0.027874012, 0.13634484, -0.03355529, -0.1006...   \n",
       "\n",
       "                                       raw_embs_idfw  labels  \\\n",
       "0  [0.03090267432090993, 0.17266593380477255, -0....       6   \n",
       "1  [0.03744219678006476, 0.20829259954964385, -0....       6   \n",
       "2  [-0.006864594470996933, 0.11324549126897607, -...       8   \n",
       "3  [-0.0077409092613638765, 0.14388482091298502, ...       4   \n",
       "4  [-0.0033498941571190985, 0.1748613285128432, -...       8   \n",
       "\n",
       "                                      norm_embs_mean  \\\n",
       "0  [0.0018537773, 0.009937191, 0.00049373374, -0....   \n",
       "1  [0.0036439227, 0.013195678, -0.0020602255, -0....   \n",
       "2  [0.0026951316, 0.0075561902, 0.0008297567, -0....   \n",
       "3  [0.0012161772, 0.0074091866, 0.0036102592, -0....   \n",
       "4  [0.002370487, 0.011595162, -0.0028536397, -0.0...   \n",
       "\n",
       "                                      norm_embs_idfw  \n",
       "0  [0.0027959669420421166, 0.015622215667215474, ...  \n",
       "1  [0.0036025583103554406, 0.020041191490469207, ...  \n",
       "2  [-0.0006629951757092065, 0.010937458097396727,...  \n",
       "3  [-0.0007445580384305126, 0.013839536984835072,...  \n",
       "4  [-0.00031487784583386625, 0.016436327794046936...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDFedt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lighGBM model with feature norm_embs_mean\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[25]\tvalid_0's multi_logloss: 2.74039\n",
      "[50]\tvalid_0's multi_logloss: 2.71338\n",
      "[75]\tvalid_0's multi_logloss: 2.68935\n",
      "[100]\tvalid_0's multi_logloss: 2.66462\n",
      "[125]\tvalid_0's multi_logloss: 2.64366\n",
      "[150]\tvalid_0's multi_logloss: 2.6228\n",
      "[175]\tvalid_0's multi_logloss: 2.60541\n",
      "[200]\tvalid_0's multi_logloss: 2.58722\n",
      "[225]\tvalid_0's multi_logloss: 2.5705\n",
      "[250]\tvalid_0's multi_logloss: 2.55461\n"
     ]
    }
   ],
   "source": [
    "embCols = [\"norm_embs_mean\", \"norm_embs_idfw\"]\n",
    "lgbmModels = [\"norm_embs_idfw\"]\n",
    "cms = []\n",
    "for col in embCols:\n",
    "    print ('Training lighGBM model with feature {}'.format(col))\n",
    "    currModel, cm = trainLGB(featureDFedt, \n",
    "             xCol = col, \n",
    "             yCol=\"labels\",\n",
    "             RANDOM_STATE = 2021,\n",
    "             num_class = 16,\n",
    "             learning_rate = 0.01,\n",
    "             num_leaves = 35,\n",
    "             min_data_in_leaf = 15,\n",
    "             feature_fraction = 0.9,\n",
    "             bagging_fraction = 0.9,\n",
    "             num_boost_round = 1000,\n",
    "             max_depth = 13,\n",
    "             bagging_freq = 5,\n",
    "             early_stopping_rounds = 20)\n",
    "    \n",
    "    lgbmModels.append(currModel)\n",
    "    cms.append(cm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
